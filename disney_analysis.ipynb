{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import numpy as np \n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Disney Metadata \n",
    "metadata_file = \"data/metadata.csv\"\n",
    "# Read our data file with the pandas library\n",
    "metadata_raw = pd.read_csv(metadata_file, encoding=\"utf-8\")\n",
    "# Show just the header to verify\n",
    "metadata_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create clean metadata Dataframe with useful columns and rename \"SPOSTMIN\" column\n",
    "metadata_clean = metadata_raw[[\"DATE\",\"WDW_TICKET_SEASON\",\"DAYOFWEEK\",\"DAYOFYEAR\",\"WEEKOFYEAR\",\n",
    "                               \"MONTHOFYEAR\",\"YEAR\",\"SEASON\",\"HOLIDAYPX\",\"HOLIDAYM\", \"HOLIDAY\",\n",
    "                               \"HOLIDAYN\"]]\n",
    "metadata_clean = metadata_clean.rename(columns={\"DATE\": \"date\", \n",
    "                                                \"HOLIDAYPX\": \"HOLIDAY_PROX\",\n",
    "                                                \"HOLIDAYM\": \"HOLIDAY_MET\",\n",
    "                                                \"HOLIDAY\": \"HOLIDAY_YN\",\n",
    "                                                \"HOLIDAYN\": \"HOLIDAY_NAME\"})\n",
    "metadata_clean = metadata_clean.drop_duplicates(subset=['date'], keep='first')\n",
    "metadata_clean.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(metadata_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Disney Kilimanjaro Safari wait times\n",
    "safari_file = \"data/kilimanjaro_safaris.csv\"\n",
    "# Read our data file with the pandas library\n",
    "safari_raw = pd.read_csv(safari_file, encoding=\"utf-8\")\n",
    "# Show just the header to verify\n",
    "safari_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create clean Dataframe with useful columns and rename \"SPOSTMIN\" column\n",
    "safari_clean = safari_raw[[\"date\", \"SPOSTMIN\"]]\n",
    "safari_clean = safari_clean.rename(columns={\"SPOSTMIN\": \"safari_wait\"})\n",
    "safari_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the rows with missing data and negative numbers\n",
    "safari_clean = safari_clean.dropna(how=\"any\")\n",
    "safari_clean = safari_clean.loc[safari_clean[\"safari_wait\"] >= 0, :]\n",
    "safari_clean.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using GroupBy in order to group the data by \"date\" values\n",
    "safari_group = safari_clean.groupby(['date'])\n",
    "safari_date = safari_group.mean()\n",
    "safari_date.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(safari_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Disney Dinosaur wait times\n",
    "dino_file = \"data/dinosaur.csv\"\n",
    "# Read our data file with the pandas library\n",
    "dino_raw = pd.read_csv(dino_file, encoding=\"utf-8\")\n",
    "# Show just the header to verify\n",
    "dino_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create clean Dataframe with useful columns and rename \"SPOSTMIN\" column\n",
    "dino_clean = dino_raw[[\"date\", \"SPOSTMIN\"]]\n",
    "dino_clean = dino_clean.rename(columns={\"SPOSTMIN\": \"dino_wait\"})\n",
    "dino_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the rows with missing data and negative numbers\n",
    "dino_clean = dino_clean.dropna(how=\"any\")\n",
    "dino_clean = dino_clean.loc[dino_clean[\"dino_wait\"] >= 0, :]\n",
    "dino_clean.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using GroupBy in order to group the data by \"date\" values\n",
    "dino_group = dino_clean.groupby(['date'])\n",
    "dino_date = dino_group.mean()\n",
    "dino_date.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dino_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge two dataframes using an outer join\n",
    "ride_wait = pd.merge(safari_date, dino_date, on=\"date\", how=\"outer\")\n",
    "ride_wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Disney Expedition Everest wait times\n",
    "everest_file = \"data/expedition_everest.csv\"\n",
    "# Read our data file with the pandas library\n",
    "everest_raw = pd.read_csv(everest_file, encoding=\"utf-8\")\n",
    "# Show just the header to verify\n",
    "everest_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create clean Dataframe with useful columns and rename \"SPOSTMIN\" column\n",
    "everest_clean = everest_raw[[\"date\", \"SPOSTMIN\"]]\n",
    "everest_clean = everest_clean.rename(columns={\"SPOSTMIN\": \"everest_wait\"})\n",
    "everest_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the rows with missing data and negative numbers\n",
    "everest_clean = everest_clean.dropna(how=\"any\")\n",
    "everest_clean = everest_clean.loc[everest_clean[\"everest_wait\"] >= 0, :]\n",
    "everest_clean.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using GroupBy in order to group the data by \"date\" values\n",
    "everest_group = everest_clean.groupby(['date'])\n",
    "everest_date = everest_group.mean()\n",
    "everest_date.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(everest_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge two dataframes using an outer join\n",
    "ride_wait = pd.merge(ride_wait, everest_date, on=\"date\", how=\"outer\")\n",
    "ride_wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Disney Avatar Flight of Passage wait times\n",
    "passage_file = \"data/flight_of_passage.csv\"\n",
    "# Read our data file with the pandas library\n",
    "passage_raw = pd.read_csv(passage_file, encoding=\"utf-8\")\n",
    "# Show just the header to verify\n",
    "passage_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create clean Dataframe with useful columns and rename \"SPOSTMIN\" column\n",
    "passage_clean = passage_raw[[\"date\", \"SPOSTMIN\"]]\n",
    "passage_clean = passage_clean.rename(columns={\"SPOSTMIN\": \"passage_wait\"})\n",
    "passage_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the rows with missing data and negative numbers\n",
    "passage_clean = passage_clean.dropna(how=\"any\")\n",
    "passage_clean = passage_clean.loc[passage_clean[\"passage_wait\"] >= 0, :]\n",
    "passage_clean.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using GroupBy in order to group the data by \"date\" values\n",
    "passage_group = passage_clean.groupby(['date'])\n",
    "passage_date = passage_group.mean()\n",
    "passage_date.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(passage_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge two dataframes using an outer join\n",
    "ride_wait = pd.merge(ride_wait, passage_date, on=\"date\", how=\"outer\")\n",
    "ride_wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Disney Navi River Journey wait times\n",
    "navi_file = \"data/navi_river.csv\"\n",
    "# Read our data file with the pandas library\n",
    "navi_raw = pd.read_csv(navi_file, encoding=\"utf-8\")\n",
    "# Show just the header to verify\n",
    "navi_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create clean Dataframe with useful columns and rename \"SPOSTMIN\" column\n",
    "navi_clean = navi_raw[[\"date\", \"SPOSTMIN\"]]\n",
    "navi_clean = navi_clean.rename(columns={\"SPOSTMIN\": \"navi_wait\"})\n",
    "navi_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the rows with missing data and negative numbers\n",
    "navi_clean = navi_clean.dropna(how=\"any\")\n",
    "navi_clean = navi_clean.loc[navi_clean[\"navi_wait\"] >= 0, :]\n",
    "navi_clean.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using GroupBy in order to group the data by \"date\" values\n",
    "navi_group = navi_clean.groupby(['date'])\n",
    "navi_date = navi_group.mean()\n",
    "navi_date.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(navi_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge two dataframes using an outer join\n",
    "ride_wait = pd.merge(ride_wait, navi_date, on=\"date\", how=\"outer\")\n",
    "ride_wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge metadata and ride wait dataframes using an outer join\n",
    "disney_data = pd.merge(metadata_clean, ride_wait, on=\"date\", how=\"outer\")\n",
    "disney_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disney_data['id']= disney_data.index\n",
    "disney_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disney_data.set_index('id', inplace = True)\n",
    "disney_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rds_connection_string = \"postgres:Blue14horse@localhost:5432/disney_data\"\n",
    "engine = create_engine(f'postgresql://{rds_connection_string}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.table_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disney_data.to_sql(name = 'disney_data', con = engine, if_exists = 'append', index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disney_sqlfinal = pd.read_sql('select * from disney_data' ,conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disney_sqlfinal = disney_sqlfinal.drop(columns = ['id'])\n",
    "disney_sqlfinal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export file as a CSV, without the Pandas index, but with the header\n",
    "disney_sqlfinal.to_csv(\"disney_sqlfinal.csv\", index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PythonAdv]",
   "language": "python",
   "name": "conda-env-PythonAdv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
